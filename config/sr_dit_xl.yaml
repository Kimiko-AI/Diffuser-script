# SR-DiT XL Training Configuration

# General Configuration
general:
  output_dir: "sr-dit-xl-training"
  seed: 42
  report_to: "wandb"
  project_name: "sr-dit-training"

# Model Configuration
model:
  model_type: "sr_dit"
  pretrained_model_name_or_path: "Tongyi-MAI/Z-Image-Turbo" # Used for VAE and Tokenizer/Text-Encoder
  text_encoder_path: "Tongyi-MAI/Z-Image-Turbo"
  vae_path: "Tongyi-MAI/Z-Image-Turbo"
  mixed_precision: "bf16"
  gradient_checkpointing: true
  
  # SR-DiT XL Parameters
  config:
    hidden_size: 1152
    depth: 28
    num_heads: 16
    patch_size: 2
    mlp_ratio: 4.0

# Training Configuration
training:
  max_train_steps: 1000000
  train_batch_size: 32 # Reduced for XL model memory requirements
  learning_rate: 0.0001
  lr_warmup_steps: 1000
  lr_scheduler_type: "constant"
  gradient_accumulation_steps: 4 # Increase accumulation for effective batch size
  max_grad_norm: 1.0
  use_ema: true
  ema_decay: 0.9999
  
  # Checkpointing
  checkpointing_steps: 2000
  checkpoints_total_limit: 5
  resume_from_checkpoint: "latest"

  # Validation / Visualization
  validation_steps: 1000
  validation_num_inference_steps: 30
  validation_guidance_scale: 5.0
  validation_prompt:
    - "A beautiful sunset over a calm ocean, cinematic lighting, 8k resolution"
    - "A futuristic city with neon lights and flying cars, digital art style"
    - "A cute white cat wearing a small crown, high quality photography"

# Data Configuration
data:
  data_url: "/workspace/shinon/anime/train/000{01..37}.tar"
  resolution: 256
  bucket_step_size: 32
  dataloader_num_workers: 8
  caption_dropout_prob: 0.1
